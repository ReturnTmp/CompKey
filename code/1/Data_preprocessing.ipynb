{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 竞争性关键词推荐算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "#### 1. 读取搜索内容  \n",
    "从比赛数据的训练集中提取出搜索记录，并以utf-8格式保存，每条记录占一行  \n",
    "搜索记录保存在以\"query_words.train\"为名称的文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version https://git-lfs.github.com/spec/v1 oid sha256:4131c60c704be000dcfd9450a427a670fea3714f700be5b7b6b91705d654c384 size 224899985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = open('./user_tag_query.10W.TRAIN','r',encoding='gb18030')\n",
    "output_data = open('./query_words.train','w',encoding='utf-8')\n",
    "for line in data:\n",
    "    print(line)\n",
    "    line_list = line.split(' ')\n",
    "    line_list = line_list[4:]\n",
    "    output_line = '\\n'.join(line_list)\n",
    "    output_data.write(output_line + '\\n')\n",
    "data.close()\n",
    "output_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 数据清洗 去除链接等非中文文本\n",
    "观察原始数据，发现文本数据条目中包含http格式的网页链接名称，与要得到的关键词无关，甚至会产生干扰，因而使用正则表达式将其去除  \n",
    "将去除后网页链接后的数据保存在以\"link_clean_data'为名称的文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train_data = open('./query_words.train','r',encoding='utf-8')\n",
    "result_data = open('./link_clean_data.train','w',encoding='utf-8')\n",
    "\n",
    "for line in train_data:\n",
    "    word_list = line.split('\\t')\n",
    "    pattern = re.compile(r'[:]?http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+') \n",
    "    if pattern.match(word_list[0]):\n",
    "        # print(word_list[0])\n",
    "        continue\n",
    "    line_string = '\\t'.join(word_list) + '\\n'\n",
    "    result_data.write(line_string)\n",
    "train_data.close()\n",
    "result_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.使用jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\86150\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.714 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "train_data = open('./link_clean_data.train','r',encoding='utf-8')\n",
    "result_data = open('./words_segmentation_data.train','w',encoding='utf-8')\n",
    "\n",
    "for sentence in train_data:\n",
    "    sentence = sentence[:-1]\n",
    "    word_seg = jieba.cut(sentence)\n",
    "    line_string = \"\\t\".join(word_seg) + '\\n'\n",
    "    result_data.write(line_string)\n",
    "train_data.close()\n",
    "result_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.过滤停用词\n",
    "文本中有很多无效的词，比如“着”，“和”，还有一些标点符号，这些都会对最后的关键词种子筛选产生干扰，因此需要去掉。  \n",
    "我们下载了一个中文停用词表，其中包含常用的停用词，我们用该表将我们文本的停用词过滤掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#停用词表文件\n",
    "stop_words = \"stop_words.txt\"\n",
    "stop_words_dict = open(stop_words, 'r', encoding='utf-8')\n",
    "stop_words_content = stop_words_dict.read()\n",
    "\n",
    "#将停用词表转换为list  \n",
    "stop_words_list = stop_words_content.splitlines()\n",
    "stop_words_dict.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open('./words_segmentation_data.train','r',encoding='utf-8')\n",
    "result_data = open('./filter_stopwords_data.train','w',encoding='utf-8')\n",
    "\n",
    "#过滤分词结果中的停用词\n",
    "def stop_words_filter(word_list,stop_words_list):\n",
    "    word_cleaned=[]\n",
    "    stopwords_list = set(stop_words_list)\n",
    "    for word in word_list:\n",
    "        if word not in stop_words_list:\n",
    "            word_cleaned.append(word)\n",
    "    return word_cleaned\n",
    "\n",
    "for line in train_data:\n",
    "    line = line[:-1]\n",
    "    word_list = line.split('\\t')\n",
    "    word_list = stop_words_filter(word_list,stop_words_list)\n",
    "    if len(word_list) == 0:\n",
    "        continue\n",
    "    line_string = \"\\t\".join(word_list) + '\\n'\n",
    "    result_data.write(line_string)\n",
    "train_data.close()\n",
    "result_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选取种子关键词  \n",
    "使用python自带的collections.Counter类进行词频统计  \n",
    "用其中的most_common()方法打印出词频出现前20的词，选择10个，作为本次项目的种子关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def read_word(filename):\n",
    "    wordlist = [];\n",
    "    data_file = open(filename,'r',encoding='utf-8')\n",
    "    for line in data_file:\n",
    "        line = line[:-1]\n",
    "        words = line.split('\\t')\n",
    "        wordlist.extend(words)\n",
    "    data_file.close()\n",
    "    return wordlist\n",
    "\n",
    "word_list = read_word('./filter_stopwords_data.train')\n",
    "count_result = Counter(word_list)\n",
    "for key, val in count_result.most_common(20):\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**选取的10个种子关键词为：图片 手机 小说 视频 下载 qq 电影 百度 英语 游戏** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 筛选还有种子关键词的搜索条目\n",
    "在选取了关键种子后，从10万条原始的搜索数据中，筛选出含有种子关键词的搜索数据  \n",
    "存在以\"seedwords_query.train\"为名称的文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open('./query_words.train','r',encoding='utf-8')\n",
    "result_data = open('./seedwords_query.train','w',encoding='utf-8')\n",
    "\n",
    "seedwords_list=['图片','手机','小说','视频','下载','qq','电影','百度','英语','游戏']\n",
    "for line in train_data:\n",
    "    flag = False\n",
    "    for seedword in seedwords_list:\n",
    "        if seedword in line:\n",
    "            flag = True\n",
    "            break\n",
    "    if(flag==True):\n",
    "        result_data.write(line)\n",
    "train_data.close()\n",
    "result_data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
